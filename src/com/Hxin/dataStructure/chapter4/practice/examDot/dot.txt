exam2:
    When rehashing, we choose a table size that is roughly twice as large and prime. In our case, the appropriate new
table size is 19, with hash function h(x) = x(mod 19).
    the following answer is based on the table size is 19,but when we look at the source code,we will find the rehash method
always get a number that is at least twice as large as the origin table size and the number is a prime.So the appropriate new
table size is 23,with the hash function h(x) = x(mod 23)
    (a)Scanning down the separate chaining hash table, the new locations are 4371 in list 1, 1323 in list 12, 6173 in
list 17, 4344 in list 12, 4199 in list 0, 9679 in list 8, and 1989 in list 13.
    (b)The new locations are 9679 in bucket 8, 4371 in bucket 1, 1989 in bucket 13, 1323 in bucket 12, 6173 in bucket 17,
4344 in bucket 14 (because both 12 and 13 are already occupied), and 4199 in bucket 0.
    (c)The new locations are 9679 in bucket 8, 4371 in bucket 1, 1989 in bucket 13, 1323 in bucket 12, 6173 in bucket 17,
4344 in bucket 16 (because both 12 and 13 are already occupied), and 4199 in bucket 0.
    (d)The new locations are 9679 in bucket 8, 4371 in bucket 1, 1989 in bucket 13, 1323 in bucket 12, 6173 in bucket 17,
4344 in bucket 15 because 12 is already occupied, and 4199 in bucket 0.

exam4:
    We must be careful not to rehash too often. Let p be the threshold (fraction of table size) at which we rehash to a
smaller table. Then, if the new table has size N, it contains 2pN elements. This table will require rehashing after
either 2N - 2pN insertions or pN deletions. Balancing these costs suggests that a good choice is p = 2/3. For instance,
suppose we have a table of size 300. If we rehash at 200 elements, then the new table size is N = 150, and we can do
either 100 insertions or 100 deletions until a new rehash is required.
    If we know that insertions are more frequent than deletions, then we might choose p to be somewhat larger. If p is
too close to 1.0, however, then a sequence of a small number of deletions followed by insertions can cause frequent
rehashing. In the worst case, if p = 1.0, then alternating deletions and insertions both require rehashing.

exam6:
    No ,this does not take deletion into account

exam7:
    If the number of deleted cells is small, then we spend extra time looking for inactive cells that are not likely to
be found. If the number of deleted cells is large, then we may get improvement.

exam8:
    Yes

exam9:
    In a good library implementation, the length method should be inlined.So the answer is Yes.

exam10:
    Separate chaining hashing requires the use of links, which costs some memory, and the standard method of implementing
calls on memory allocation routines, which typically are expensive. Linear probing is easily implemented, but performance
degrades severely as the load factor increases because of primary clustering. Quadratic probing is only slightly more
difficult to implement and gives good performance in practice. An insertion can fail if the table is half empty,
but this is not likely. Even if it were, such an insertion would be so expensive that it wouldn't matter and would almost
certainly point up a weakness in the hash function. Double hashing eliminates primary and secondary clustering, but the
computation of a second hash function can be costly. there are famous experts suggest that quadratic probing is the fastest method.

exam12:
    The old values would remain valid if the hashed values were less than the old table size.

exam13:
    Sorting the MN records and eliminating duplicates would require O(MN log MN) time using a standard sorting algorithm.
If terms are merged by using a hash function, then the merging time is constant per term for a total of O(MN). If the
output polynomial is small and has only O(M + N) terms, then it is easy to sort it in O((M + N) log (M + N)) time,
which is less than O(MN). Thus, the total is O(MN). This bound is better because the model is less restrictive: Hashing
is performing operations on the keys rather than just comparison between the keys. A similar bound can be obtained by
using bucket sort instead of a standard sorting algorithm. Operations such as hashing are much more expensive than
comparisons in practice, so this bound might not be an improvement. On the other hand, if the output polynomial is
expected to have only O(M + N) terms, then using a hash table saves a huge amount of space, since under these conditions,
the hash table needs only O(M + N) space.
    Another method of implementing these operations is to use a search tree instead of a hash table; a balanced tree is
required because elements are inserted in the tree with too much order. A splay tree might be particularly well suited
for this type of problem because it does well with sequential accesses. Comparing the different ways of solving the
problem is a good programming assignment.

exam14:
    To each hash table slot, we can add an extra data member that weâ€™ll call whereOnStack, and we can keep an extra stack.
When an insertion is first performed into a slot, we push the address (or number) of the slot onto the stack and set the
whereOnStack data member to refer to the top of the stack. When we access a hash table slot, we check that whereOnStack
refers to a valid part of the stack and that the entry in the (middle of the) stack that is referred to by the whereOnStack
data member has that hash table slot as an address.

